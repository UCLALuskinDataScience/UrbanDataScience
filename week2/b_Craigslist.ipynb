{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sunrise-lease",
   "metadata": {},
   "source": [
    "# Week 2. Web scraping (continued)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-watts",
   "metadata": {},
   "source": [
    "## Example: Scraping Craigslist data\n",
    "Craiglist provides a wealth of information on apartment rentals and other types of housing (as we saw in the [Boeing and Waddell paper](https://journals.sagepub.com/doi/abs/10.1177/0739456X16664789)). But short of clicking through lots of links, how do we access it?\n",
    "\n",
    "As with any scraping project, the first step is to get an example web page, and see if we can reverse-engineer the structure.\n",
    "\n",
    "One option is to parse each detailed post, with information on parking, desired qualities of roommates, etc. But a lot of information is actually in the [list of posts](https://losangeles.craigslist.org/search/lac/hhh). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-maker",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://losangeles.craigslist.org/search/lac/hhh'\n",
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-recycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.content)\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charming-pricing",
   "metadata": {},
   "source": [
    "It looks like each post is in a `<li>` tag. Moreover, note that it's also in a `class` called `result-row`. Structured data like this make it much easier to scrape! The `find_all()` function takes an optional `class_` argument that can filter by class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-experience",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = soup.find_all('li', class_= 'result-row')\n",
    "\n",
    "# Note that there are 120 results, which is the number of posts returned on the Craigslist webpage. A good thing!\n",
    "print(len(posts))\n",
    "\n",
    "# Let's look at a sample post\n",
    "posts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tight-colors",
   "metadata": {},
   "source": [
    "It looks like the price and the neighborhood have their own class, within the `span` tag. \n",
    "The title and URL look like they are within the `a` tag. The number of bedrooms is a bit more complicated, but it's somewhere in the housing class.\n",
    "\n",
    "Let's test this out. Note that `find` will get the first occurence. `find_all` will get all of them, and return a list. But in the CraigsList posts, there's only either one occurence or they are all the same, so `find` is easier. (Try it out.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-filling",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Price:')\n",
    "print(posts[0].find('span', class_= 'result-price'))\n",
    "\n",
    "print('\\nNeighborhood:') # \\n adds an empty line before\n",
    "print(posts[0].find('span', class_= 'result-hood'))\n",
    "\n",
    "print('\\nHousing size:')\n",
    "print(posts[0].find('span', class_= 'housing'))\n",
    "\n",
    "print('\\nTitle:')\n",
    "print(posts[0].find('a', class_= 'result-title'))\n",
    "\n",
    "# For all of these results, we can extract just the text\n",
    "print('\\nTitle  (text only):')\n",
    "print(posts[0].find('a', class_= 'result-title').text)\n",
    "\n",
    "# except the URL has it's own key\n",
    "print('\\nURL:')\n",
    "print(posts[0].find('a', class_= 'result-title')['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-romania",
   "metadata": {},
   "source": [
    "Now we understand the structure of each page. So we are ready to put all of the posts in a dataframe.\n",
    "\n",
    "`pandas` can create a dataframe from many different data structures. But one of the easiest ways to is to create a list of dictionaries, and then tell `pandas` to convert that into a dataframe. The list is of rows. Within each list, we have a dictionary of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-expansion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "postList = [] # empty list that we can add to\n",
    "for post in posts:\n",
    "    # temporary variables\n",
    "    price = post.find('span', class_= 'result-price').text\n",
    "    neighborhood = post.find('span', class_= 'result-hood').text\n",
    "    housingsize = post.find('span', class_= 'housing').text\n",
    "    title = post.find('a', class_= 'result-title').text\n",
    "    url = post.find('a', class_= 'result-title')['href']\n",
    "\n",
    "    # now put them in the dictionary, and append to our list\n",
    "    postList.append({'price': price, 'neighborhood':neighborhood, \n",
    "                     'housingsize':housingsize, 'title':title, 'url':url})\n",
    "df = pd.DataFrame(postList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-testing",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "We probably got an error there. Let's discuss how to fix this to be more robust to missing fields.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-committee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normal-index",
   "metadata": {},
   "source": [
    "So it looks pretty good, except for the `housingsize` field. What's going on here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-apartment",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.housingsize)\n",
    "\n",
    "print('\\nThe first entry is {}'.format(df.housingsize.iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-afternoon",
   "metadata": {},
   "source": [
    "It looks like there is a lot of whitespace here. And sometimes, the field contains ft2, sometimes br, sometimes neither and sometimes both.\n",
    "\n",
    "Let's use the `split()` function to split the string by the whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-treat",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.housingsize.str.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-wright",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> Conceptually, how would you go about creating two new fields in the dataframe—bedrooms and sqft? Write some code if you can, but the most important step is to think through how you'd do it in words.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closing-edward",
   "metadata": {},
   "source": [
    "Now let's plot the distribution of price. A box plot would be a good choice here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-instruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oops. What went wrong?\n",
    "df.boxplot('price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-temperature",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price_numeric'] = df.price.str.replace('$','').str.replace(',','').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-married",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot('price_numeric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-adelaide",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also break it out by neighborhood.\n",
    "# But what's the problem here?\n",
    "df.boxplot('price_numeric', by='neighborhood')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about the relationship between prices and the apartment size?\n",
    "#df.plot('price_numeric', 'price_numeric')\n",
    "df.plot('sqft', 'price_numeric', kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-environment",
   "metadata": {},
   "source": [
    "So now we've created a dataframe that extracts all the posts on the first page!\n",
    "\n",
    "What next?\n",
    "* We only have one page, and it would be useful to get data from the subsequent pages\n",
    "* Our neighborhood field is really dirty, so it's hard to do any mapping\n",
    "* We don't have any information about parking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-jewel",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> How might you implement one or more of these extensions? Before writing any code, sketch out the principle and sequence of steps that you would follow.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-course",
   "metadata": {},
   "source": [
    "Let's briefly see what it would take to get the information on a specific webpage. \n",
    "\n",
    "Note that we had the foresight to save the URL in the DataFrame that we created above. Let's take the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-semester",
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = df.iloc[0]\n",
    "url = 'https://losangeles.craigslist.org/lac/roo/d/pasadena-top-floor-townhouse-master/7300581598.html'\n",
    "r = requests.get(url)\n",
    "txt = r.text\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-communication",
   "metadata": {},
   "source": [
    "We have a couple of strategies here. First, we could skip trying to parse the page with `BeautifulSoup`, and just see if particular bits of text are present.\n",
    "\n",
    "For example, what transportation modes does the post emphasize? Do they mention Section 8 vouchers? Some of this might be exploratory—we can see what type of language is included, and then parse in a more structured way (e.g. distinguishing between \"No Section 8\" and \"Section 8 welcome\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-briefing",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'freeway' in txt:\n",
    "    print('This post mentions freeways')\n",
    "if 'transit' in txt or 'train' in txt or 'bus' in txt:\n",
    "        print('This post mentions transit')\n",
    "if 'section 8' in txt:\n",
    "        print('This post mentions Section 8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-replacement",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> Write a function that takes a URL as its argument, and returns 3 boolean values for whether a post mentions freeways, transit, and Section 8. Make sure that it is not case-sensitive!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-carol",
   "metadata": {},
   "source": [
    "Most of the post is free-form text. So there's not going to be much value added by `BeautifulSoup`.\n",
    "\n",
    "The exception is the geographic coordinates, which look like they are in a `div` tag and a `viewposting` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-calculator",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.content)\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-cartoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "latlon = soup.find('div', class_='viewposting')\n",
    "lat = latlon['data-latitude']\n",
    "lon = latlon['data-longitude']\n",
    "print(lat, lon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-boring",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>Key Takeaways</h3>\n",
    "<ul>\n",
    "  <li>Scraping unstructured webpages involves more detective work and trial and error.</li>\n",
    "  <li>Some will have a consistent format and helpful class codes and html tags. Some won't.</li>\n",
    "  <li>Your code will need to be robust to missing fields and other inconsistencies in page formatting.</li>\n",
    "  <li>Be nice! You may need to slow the pace of your requests down.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
