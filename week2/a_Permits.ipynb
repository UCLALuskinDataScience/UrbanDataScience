{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sunrise-lease",
   "metadata": {},
   "source": [
    "# Week 2. Web scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-train",
   "metadata": {},
   "source": [
    "## This week's objectives\n",
    "\n",
    "1. Understand how to scrape web pages and other data where an API doesn't exist\n",
    "2. Introduce the `BeautifulSoup` library\n",
    "3. Learn how to parse unstructured text data\n",
    "4. Learn how to handle errors (\"exceptions\") gracefully\n",
    "5. More pratice with `pandas`, `geopandas`, and plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-fantasy",
   "metadata": {},
   "source": [
    "APIs make it relatively simple to get data from the web. But sometimes, an API doesn't exist—they take effort on the part of the agency to set up and maintain.\n",
    "\n",
    "In these cases, we can still obtain data from the web. But rather than dropping it directly into a (geo)pandas `DataFrame`, we'll need to do more work to understand the structure of the webpage, and to clean and process the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-newton",
   "metadata": {},
   "source": [
    "## Example: Land use permit data\n",
    "Often, cities make their building and land use permit data available for download, and/or accessible through an API. But these are typically incomplete—they provide a subset of fields that are most relevant to most users (e.g., permit approval date and number of units), but perhaps exclude more esoteric fields. And parking, sadly, is one of the fields that is often excluded.\n",
    "\n",
    "For a recent project, I looked at the impacts of TOD plans in Seattle and San Francisco on development outcomes, including parking ratios. Let's walk through the Seattle analysis.\n",
    "\n",
    "The [Seattle land use permit dataset is here](https://data.seattle.gov/Permitting/Land-Use-Permits/ht3q-kdvx). Let's get this into a `pandas` dataframe, in the same way that we did with the LA data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-console",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "url = 'https://data.seattle.gov/resource/ht3q-kdvx.json' # copied and pasted from the webpage\n",
    "r = requests.get(url)\n",
    "df = pd.DataFrame(json.loads(r.text))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-pension",
   "metadata": {},
   "source": [
    "Notice a couple of things. First, the website seems to have more fields than are in the API version of the document. Second, parking is nowhere to be seen.\n",
    "\n",
    "But there is a `link` field. Let's take a look at the first one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-bundle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The .loc operator gives us an extract from the dataframe. 0 is the row index, 'link' is the column\n",
    "\n",
    "print(df.loc[0,'link'])   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-fisher",
   "metadata": {},
   "source": [
    "Notice that this column of the pandas dataframe is a dictionary. That's perhaps a surprise, but we know how to deal with dictionaries. \n",
    "\n",
    "For now, [let's take a look at what this link looks like](https://cosaccela.seattle.gov/portal/customize/LinkToRecord.aspx?altId=3003094-LU). Clearly, there is a lot more information here about the specific permit!\n",
    "\n",
    "How do we bring the information in that webpage into Python? Remember, the `requests` library is our friend in this circumstance. While we've used it to get data from an API, `requests` can retrieve pretty much anything from the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-hollywood",
   "metadata": {},
   "outputs": [],
   "source": [
    "urldict = df.loc[0,'link']\n",
    "permiturl = urldict['url']\n",
    "# or we could do this in one step: df.loc[0,'link']['url']\n",
    "r = requests.get(permiturl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-remove",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at what the r object has given us. \n",
    "# Remember, the .text attribute gives us the text of what's retrieved.\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-thirty",
   "metadata": {},
   "source": [
    "It looks like we've got the whole .html webpage. The relevant information is buried in there, but how can we get it in the sea of html code?\n",
    "\n",
    "This is where the `BeautifulSoup` library comes in ([documentation here](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)). Let's convert our text to a \"soup\" object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-pencil",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(r.text)\n",
    "print(type(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focal-mobility",
   "metadata": {},
   "source": [
    "This soup object has a lot of attributes and functions (type `soup.` and press tab to autocomplete). We'll use the `.find` function to find the relevant text.\n",
    "\n",
    "We can also use the `.prettify()` function to give us a better sense of what we are looking at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-perfume",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not very pretty IMHO, but we can look at see where the data we want are buried\n",
    "# and cross-refernce that to the webpage in our browser\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-thomas",
   "metadata": {},
   "source": [
    "Let's suppose we want to get information the project description (where the parking information might be included, since there isn't a separate parking field). \n",
    "\n",
    "It looks like this field are contained within a `<td>` tag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-frost",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = soup.find_all('td') # returns a \"list-like\" object, i.e. we can loop through it or slice it like a list\n",
    "\n",
    "# Look at an example\n",
    "print(links[5])\n",
    "\n",
    "# More systematically, let's look at the links that we are interested\n",
    "for link in links:\n",
    "    if 'Project Description' in link.text: \n",
    "        print (link) \n",
    "        break # so that we keep this link, and abort the loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-preserve",
   "metadata": {},
   "source": [
    "Now we are getting closer! It looks like the Project Description is contained in another `<td>` tag, nested one level down. So let's do the same thing again at this second-level link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-messaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "sublinks = link.find_all('td')\n",
    "print(sublinks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-frank",
   "metadata": {},
   "source": [
    "We've obtained a list! And the information we need is in the second element of that list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-hygiene",
   "metadata": {},
   "outputs": [],
   "source": [
    "description = sublinks[1]\n",
    "print(description.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-covering",
   "metadata": {},
   "source": [
    "Now, let's take everything we've done so far, and put it in a function.\n",
    " \n",
    "The function takes a single argument: the dictionary in the `url` column of the pandas DataFrame\n",
    " \n",
    "It returns the Description text, unless that's not found, in which case it returns an empty string `''`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-functionality",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDescription(urldict):\n",
    "    permiturl = urldict['url']\n",
    "    # or we could do this in one step: df.loc[0,'link']['url']\n",
    "    r = requests.get(permiturl)\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    links = soup.find_all('td')\n",
    "    for link in links:\n",
    "        if 'Project Description' in link.text: \n",
    "            sublinks = link.find_all('td')\n",
    "            description = sublinks[1].text\n",
    "            # once we find a description, we return it and exit the function\n",
    "            return description \n",
    "    \n",
    "    return '' # if we don't find it, return an empty string\n",
    "\n",
    "urldict = df.loc[0,'link']\n",
    "getDescription(urldict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-prompt",
   "metadata": {},
   "source": [
    "The advantage of a function is that we can now apply this procedure to every row of our pandas DataFrame.\n",
    "\n",
    "Let's do this for 5 rows (so we are nice and don't disrupt the City's website).\n",
    "\n",
    "The `apply` function in `pandas` applies a function to each row of a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-inflation",
   "metadata": {},
   "outputs": [],
   "source": [
    "smalldf = df.iloc[:5].copy()  # create a copy, rather than a view to that object\n",
    "descriptions = smalldf['link'].apply(getDescription)  # for each row in smallDf, we pass the link column to getDescription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-patch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what's the description object? It's a pandas Series (basically, a one-column DataFrame)\n",
    "print(type(descriptions))\n",
    "print(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-noise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we can insert that into the dataframe as a new column\n",
    "smalldf['description'] = descriptions\n",
    "# we could have done this in one step: \n",
    "# smalldf['description'] = smalldf['link'].apply(getDescription) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-scotland",
   "metadata": {},
   "source": [
    "Now we have scraped the description for each project!\n",
    "\n",
    "How do we get the number of parking spaces? Well, that depends on whether the city uses consistent terminology. \n",
    "\n",
    "For starters, let's just get a column for whether there is \"no parking\" in the description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-digest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def noparking(description):\n",
    "    if isinstance(description, str):\n",
    "        text = description.lower()\n",
    "        if 'no parking' in text:\n",
    "            return True\n",
    "        elif 'parking' in text:\n",
    "            return False\n",
    "    return np.nan\n",
    "smalldf['noparking'] = smalldf.description.apply(noparking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-advocate",
   "metadata": {},
   "outputs": [],
   "source": [
    "smalldf.noparking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-mistress",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> If you want to get the number of parking spaces for each project, what would be your next step? In principle, how might you do that?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-trout",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Let's generalize.</strong> What did we do here?\n",
    "    \n",
    "1. We obtained the URL for each page to scrape. (Here, it was given to us in the city's data file, but sometimes we'll have to reverse-engineer the composition of the URL.)\n",
    "2. We examined a sample page, and identified the html tags that enclose the data we wanted to extract.\n",
    "3. We wrote a function that pulled out the data for a specific page.\n",
    "4. We applied that function to each URL / page. Since our URLs were in a pandas DataFrame, we could use the pandas <strong>apply</strong> method.\n",
    "    \n",
    "Every scraping project will pose different challenges, but normally it will involve each of these four steps.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
